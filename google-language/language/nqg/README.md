# Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?

This directory contains code related to the paper "Compositional Generalization
and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"
(Peter Shaw, Ming-Wei Chang, Panupong Pasupat, Kristina Toutanova).

The current version of this library contains code for reproducing the dataset
splits used in the paper, and generating new splits.

## Datasets

Below are instructions for reproducing the dataset splits used in the paper.

We use a standard TSV format for representing all splits, where each
line corresponds to an example and is formatted as:

`<source>\t<target>\n`

Where `<source>` is the input string and `<target>` is the output string.

### SCAN

The "add primitive" and "length" splits, as well as the original dataset,
are available here:

https://github.com/brendenlake/SCAN

Instructions to produce the SCAN MCD splits are here:

https://github.com/google-research/google-research/tree/master/cfq#scan-mcd-splits

The SCAN files can be converted to our TSV format by using the
`tasks/scan/convert_to_tsv.py` script for files in the original dataset format,
and `tasks/scan/join_txt_to_tsv.py` to join the input and output txt files
generated for the MCD splits.

### GeoQuery

You can learn more about the GeoQuery dataset here:
https://www.cs.utexas.edu/users/ml/nldata/geoquery.html

You can download the GeoQuery corpus with FunQL annotated expressions here:
http://www.cs.utexas.edu/~ml/wasp/geo-funql/corpus.xml

You will also need to download the geobase file which is used to identify
entities to replace with placeholders:
[ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geobase](ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geobase)

You can then generate the dataset in TSV format using the
`tasks/geoquery/write_dataset.py` script.

Example usage:

```shell
nqg/tasks/geoquery/write_dataset.py \
--corpus=/path/to/.../geoquery.xml \
--geobase=/path/to/.../geobase \
--output=/path/to/.../dataset.tsv
```


You can then reproduce the four splits used in the paper using the
`tasks/split_dataset.py` script, with the output of `write_dataset.py`
as `--input` and a file in `tasks/geoquery/splits` as `--split`.


### Spider

The Spider dataset can be downloaded from this location:
https://yale-lily.github.io/spider

Below, we will assume that the environment variable `SPIDER_DIR` points to a
directory that contains the Spider dataset.

In the paper we use various splits of a dataset we refer to as Spider-SSP which
contains all examples in the original Spider training set for databases with
at least 50 examples. This set of databases can be determined using the
`tasks/spider/print_database_counts.py` script, but is also hardcoded in
`tasks/spider/database_constants.py`.

You can generate the Spider-SSP dataset in TSV format using the
`tasks/spider/write_dataset.py` script with
`--examples=${SPIDER_DIR}/data/train_spider.json`.


When using T5, we append a serialized database schema to the input string.
This can be accomplished using the `tasks/spider/append_schema.py` script with
`--input` set to the TSV generated by `tasks/spider/write_dataset.py` and
`--tables=${SPIDER_DIR}/data/tables.json`.


You can then reproduce the four splits used in the paper using the
`tasks/split_dataset.py` script, with the output of `write_dataset.py` or
`append_schema.py` as `--input` and a file in `tasks/spider/splits` as
`--split`.


For evaluation, you will need to download the Spider `evaluation.py` script
from: https://github.com/taoyds/spider.

You can then use `tasks/spider/generate_gold.py` to generate an input file
for the `--gold` flag of `evaluation.py`. The `--preds` flag should point to a
txt file of generated predictions.

Note that for T5, you will need to run `tasks/spider/restore_oov.py` to
post-process generated predictions.

### Generating New Dataset Splits

For generating new random and length splits, the `tasks/gen_length_split.py`
and `tasks/gen_random_split.py` tools can be applied to any dataset in the TSV
format described above. We describe how to generate new template and TMCD
splits below.

#### Template Splits

For generating new template splits for GeoQuery using the same template
definition used in the paper, you can use the
`tasks/geoquery/gen_template_split.py` tool, with the output of
`tasks/geoquery/write_dataset.py` as `--input`.

Similarly, for Spider, you can use the `tasks/spider/gen_template_split.py`
tool, with the output of `tasks/spider/write_dataset.py` or
`tasks/spider/append_schema.py` as `--input`.

For generating template splits for new datasets, or for generating template
splits for the datasets studied in this work using a different procedure to
determine target templates, you can use the utilities in
`tasks/template_utils.py`.

#### TMCD Splits

To apply the TMCD methodology to new datasets, or to change the definition
of atoms and compounds for the dataset studied in this paper, you may find
the utilities in `tasks/mcd_utils.py` useful. The functions in this module
require you to define functions to map examples to atoms and compounds,
but you can then generate new TMCD splits and compute various metrics.

You can find several scripts related to TMCD and the corresponding notions
of compound divergence in both the `tasks/geoquery` and
`tasks/spider` sub-directories:

* `gen_tmcd_split.py` - Tool to generate a new TMCD split, given a dataset in TSV format.
* `measure_compound_divergence.py` - Tool to measure compound divergence for any split.
* `measure_unseen_atoms.py` - Tool to measure the number of examples containing unseen atoms for any split.

Note that there may also be interest in applying a different atom constraint,
e.g. a constraint based on atom divergence (similarly to Keysers et al. 2020)
as opposed to the constraint used in the paper, depending on the focus of
a given evaluation and the size of the datasets being considered.
Unfortunately, the code does not currently support
this functionality, but ideally the utilities in `tasks/mcd_utils.py` can
provide a helpful starting point.

Also note that definition of compounds for Spider released in this library uses
a slightly simplified and significantly more readable CFG for parsing SQL than
the grammar used to define compounds for the original paper, which we therefore
believe to be more useful for future work. Please contact the authors if you
have a specific need for the original compound definition.

## Approaches

### T5

Instructions for fine-tuning T5 given a dataset in the TSV format described
above are here:

https://github.com/google-research/text-to-text-transfer-transformer#using-a-tsv-file-directly

This document also contains instructions for generating predictions:

https://github.com/google-research/text-to-text-transfer-transformer#decode

To create a txt file with inputs to generate test predictions from a test tsv
file, you can use the `tasks/strip_targets.py` script. For datasets using
simple exact match (GeoQuery and SCAN), you can then compare these predictions
with the targets provided by a TSV file for a given test split using the script
`tasks/compare_predictions.py`.
