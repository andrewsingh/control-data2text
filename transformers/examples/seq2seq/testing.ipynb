{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b459e1ccd854e432e4dd220178642c3c94442aa125d1d3c88814638f54b7d686"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    T5Tokenizer\n",
    ")\n",
    "from sacremoses import MosesDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/projects/ogma2/users/andrewsi/control-data2text\"\n",
    "model_path = f\"{root_dir}/transformers/examples/language-modeling/exp/e2e_targets/gpt2-01/checkpoint-7458\"\n",
    "gpu = \"3\"\n",
    "\n",
    "def compute_perplexity(preds):\n",
    "    e2e_lm_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    e2e_lm = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    e2e_lm.to(f\"cuda:{gpu}\")\n",
    "    ppls = []\n",
    "    for pred in tqdm(preds):\n",
    "        inputs = e2e_lm_tokenizer(pred, return_tensors='pt').to(\"cuda:3\")\n",
    "        outputs = e2e_lm(**inputs, labels=inputs['input_ids'])\n",
    "        ppls.append(math.exp(outputs.loss))\n",
    "    return round((sum(ppls) / len(ppls)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = MosesDetokenizer(lang='en')\n",
    "\n",
    "def process_e2e_preds(inpath, outpath=None):\n",
    "    processed_lines = []\n",
    "    with open(inpath, \"r\") as f:\n",
    "        original_lines = [line for line in f]\n",
    "    for line in tqdm(original_lines):\n",
    "        processed_lines.append(md.detokenize(line.strip().replace(\"_\", \" \").split()))\n",
    "    if outpath:\n",
    "        with open(outpath, \"w+\") as f:\n",
    "            f.writelines([line + \"\\n\" for line in processed_lines])\n",
    "    return processed_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/projects/ogma2/users/andrewsi/control-data2text/DTG-SI/e2e_data/train/y_aux.train.txt\"\n",
    "val_file = \"/projects/ogma2/users/andrewsi/control-data2text/DTG-SI/e2e_data/val/y_aux.valid.txt\"\n",
    "test_file = \"/projects/ogma2/users/andrewsi/control-data2text/DTG-SI/e2e_data/test/y_aux.test.txt\"\n",
    "outdir = \"/projects/ogma2/users/andrewsi/control-data2text/transformers/examples/language-modeling/test_data/e2e_targets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6274/6274 [00:04<00:00, 1565.00it/s]\n"
     ]
    }
   ],
   "source": [
    "process_e2e_preds(test_file, f\"{outdir}/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_ppl(inpath):\n",
    "    return compute_perplexity(process_e2e_preds(inpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}