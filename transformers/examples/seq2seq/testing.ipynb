{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b459e1ccd854e432e4dd220178642c3c94442aa125d1d3c88814638f54b7d686"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    T5Tokenizer\n",
    ")\n",
    "from sacremoses import MosesDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/projects/ogma2/users/andrewsi/control-data2text\"\n",
    "model_path = f\"{root_dir}/transformers/examples/language-modeling/exp/e2e_targets/gpt2-02/checkpoint-9464\"\n",
    "gpu = \"0\"\n",
    "\n",
    "def compute_perplexity(preds):\n",
    "    e2e_lm_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    e2e_lm = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    e2e_lm.to(f\"cuda:{gpu}\")\n",
    "    ppls = []\n",
    "    print(f\"First pred: {preds[0]}\")\n",
    "    for pred in tqdm(preds):\n",
    "        inputs = e2e_lm_tokenizer(pred, return_tensors='pt').to(f\"cuda:{gpu}\")\n",
    "        outputs = e2e_lm(**inputs, labels=inputs['input_ids'])\n",
    "        ppls.append(math.exp(outputs.loss))\n",
    "    return round((sum(ppls) / len(ppls)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = MosesDetokenizer(lang='en')\n",
    "\n",
    "def process_e2e_preds(inpath, outpath=None):\n",
    "    processed_lines = []\n",
    "    with open(inpath, \"r\") as f:\n",
    "        original_lines = [line for line in f]\n",
    "    for line in tqdm(original_lines):\n",
    "        processed_lines.append(md.detokenize(line.strip().replace(\"_\", \" \").split()))\n",
    "    if outpath:\n",
    "        with open(outpath, \"w+\") as f:\n",
    "            f.writelines([line + \"\\n\" for line in processed_lines])\n",
    "    return processed_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/projects/ogma2/users/andrewsi/control-data2text/DTG-SI/e2e_data/train/y_aux.train.txt\"\n",
    "val_file = \"/projects/ogma2/users/andrewsi/control-data2text/DTG-SI/e2e_data/val/y_aux.valid.txt\"\n",
    "test_file = \"/projects/ogma2/users/andrewsi/control-data2text/DTG-SI/e2e_data/test/y_aux.test.txt\"\n",
    "outdir = \"/projects/ogma2/users/andrewsi/control-data2text/transformers/examples/language-modeling/test_data/e2e_targets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6274/6274 [00:04<00:00, 1565.00it/s]\n"
     ]
    }
   ],
   "source": [
    "process_e2e_preds(test_file, f\"{outdir}/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_ppl(inpath):\n",
    "    return compute_perplexity(process_e2e_preds(inpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6300/6300 [00:05<00:00, 1250.52it/s]\n",
      "  0%|          | 0/6300 [00:00<?, ?it/s]First pred: Loch Fyne in riverside near The Rice Boat has a high customer rating. They serve French food.\n",
      "100%|██████████| 6300/6300 [02:29<00:00, 42.02it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4.102"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "get_new_ppl(\"/projects/ogma2/users/andrewsi/control-data2text/transformers/examples/seq2seq/exp/e2e/e2e_k3_t5_small_01/checkpoint-8295/validation_preds.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prop_longer(col, thresh):\n",
    "    return len(col[col > thresh]) / len(col)\n",
    "\n",
    "def get_len_df(data_file):\n",
    "    with open(data_file, \"r\") as f:\n",
    "        data_lines = [line for line in f]\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=4096)\n",
    "    special_tokens = [\"[SEP]\"]\n",
    "        \n",
    "    if len(special_tokens) > 0:\n",
    "        special_tokens_dict = {\"additional_special_tokens\": (special_tokens)}\n",
    "        tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    print(\"\\nTokenizer length: {}\".format(len(tokenizer)))\n",
    "    \n",
    "    src_lens = []\n",
    "    tgt_lens = []\n",
    "    print(f\"Num lines: {len(data_lines)}\\nFirst line: {data_lines[0]}\")\n",
    "    for line in tqdm(data_lines):\n",
    "        json_example = json.loads(line)\n",
    "        src_lens.append(len(tokenizer(json_example[\"source\"], max_length=4096, truncation=True)['input_ids'])) \n",
    "        tgt_lens.append(len(tokenizer(json_example[\"target\"], max_length=4096, truncation=True)['input_ids']))\n",
    "\n",
    "    return pd.DataFrame([src_lens, tgt_lens], index=[\"src_len\", \"tgt_len\"]).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}